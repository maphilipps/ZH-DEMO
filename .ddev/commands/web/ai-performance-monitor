#!/usr/bin/env bash

## #ddev-generated
## Description: Monitor and optimize AI integration performance
## Usage: ai-performance-monitor [options]
## Example: "ddev ai-performance-monitor --test-all"
##   or "ddev ai-performance-monitor --provider=openai"
##   or "ddev ai-performance-monitor --benchmark --cache-test"
##   or "ddev ai-performance-monitor --optimize --report=/tmp/ai-perf.json"

set -e

# Color output functions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${BLUE}[AI-PERF]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_ai() {
    echo -e "${MAGENTA}[AI]${NC} $1"
}

# AI performance monitoring settings
AI_PERF_DIR="/tmp/adesso-cms-ai-perf"
AI_CACHE_DIR="/var/www/html/.ai-performance-cache"
REPORT_FILE=""

# Parse command line arguments
TEST_ALL=false
PROVIDER=""
BENCHMARK_MODE=false
CACHE_TEST=false
OPTIMIZE_MODE=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --test-all)
            TEST_ALL=true
            shift
            ;;
        --provider=*)
            PROVIDER="${1#*=}"
            shift
            ;;
        --benchmark)
            BENCHMARK_MODE=true
            shift
            ;;
        --cache-test)
            CACHE_TEST=true
            shift
            ;;
        --optimize)
            OPTIMIZE_MODE=true
            shift
            ;;
        --report=*)
            REPORT_FILE="${1#*=}"
            shift
            ;;
        *)
            log_error "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Create directories
mkdir -p "$AI_PERF_DIR" "$AI_CACHE_DIR"

# Initialize report if specified
if [ -n "$REPORT_FILE" ]; then
    cat > "$REPORT_FILE" << 'EOF'
{
  "timestamp": "",
  "ai_providers": {},
  "performance_tests": {},
  "cache_performance": {},
  "optimizations": [],
  "recommendations": []
}
EOF
    
    # Update timestamp
    python3 -c "
import json
from datetime import datetime

with open('$REPORT_FILE', 'r') as f:
    data = json.load(f)

data['timestamp'] = datetime.utcnow().isoformat() + 'Z'

with open('$REPORT_FILE', 'w') as f:
    json.dump(data, f, indent=2)
"
fi

# Function to detect AI providers
detect_ai_providers() {
    log_info "Detecting AI provider configurations..."
    
    local providers=()
    local provider_status=()
    
    # Check OpenAI
    if drush config:get ai_provider_openai.settings 2>/dev/null >/dev/null; then
        providers+=("openai")
        local api_key=$(drush config:get ai_provider_openai.settings api_key 2>/dev/null | sed 's/.*: //' | tr -d "'" || echo "")
        if [ -n "$api_key" ] && [ "$api_key" != "null" ]; then
            provider_status+=("configured")
        else
            provider_status+=("missing_key")
        fi
    fi
    
    # Check Anthropic
    if drush config:get ai_provider_anthropic.settings 2>/dev/null >/dev/null; then
        providers+=("anthropic")
        local api_key=$(drush config:get ai_provider_anthropic.settings api_key 2>/dev/null | sed 's/.*: //' | tr -d "'" || echo "")
        if [ -n "$api_key" ] && [ "$api_key" != "null" ]; then
            provider_status+=("configured")
        else
            provider_status+=("missing_key")
        fi
    fi
    
    # Check Groq
    if drush config:get ai_provider_groq.settings 2>/dev/null >/dev/null; then
        providers+=("groq")
        local api_key=$(drush config:get ai_provider_groq.settings api_key 2>/dev/null | sed 's/.*: //' | tr -d "'" || echo "")
        if [ -n "$api_key" ] && [ "$api_key" != "null" ]; then
            provider_status+=("configured")
        else
            provider_status+=("missing_key")
        fi
    fi
    
    # Save provider information
    cat > "$AI_PERF_DIR/providers.json" << EOF
{
  "detected_providers": $(printf '%s\n' "${providers[@]}" | jq -R . | jq -s .),
  "provider_status": $(printf '%s\n' "${provider_status[@]}" | jq -R . | jq -s .),
  "total_providers": ${#providers[@]}
}
EOF
    
    log_success "Detected ${#providers[@]} AI providers: ${providers[*]}"
    
    # Update report if specified
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

with open('$AI_PERF_DIR/providers.json', 'r') as f:
    provider_data = json.load(f)

report_data['ai_providers'] = provider_data

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
}

# Function to test AI provider performance
test_ai_provider_performance() {
    local provider="$1"
    
    log_ai "Testing $provider provider performance..."
    
    local start_time=$(date +%s.%N)
    local test_results="{"
    
    # Test provider configuration loading
    local config_start=$(date +%s.%N)
    local config_loaded=false
    
    if drush config:get "ai_provider_${provider}.settings" >/dev/null 2>&1; then
        config_loaded=true
    fi
    
    local config_end=$(date +%s.%N)
    local config_duration=$(echo "$config_end - $config_start" | bc -l)
    
    test_results+='"config_load_time": '$config_duration','
    test_results+='"config_loaded": '$config_loaded','
    
    # Test AI module status
    local module_start=$(date +%s.%N)
    local ai_modules_enabled=$(drush pm:list --filter="ai" --status=enabled --format=json 2>/dev/null | python3 -c "
import json
import sys
try:
    data = json.load(sys.stdin)
    print(len(data))
except:
    print('0')
" || echo "0")
    local module_end=$(date +%s.%N)
    local module_duration=$(echo "$module_end - $module_start" | bc -l)
    
    test_results+='"module_check_time": '$module_duration','
    test_results+='"ai_modules_enabled": '$ai_modules_enabled','
    
    # Test cache operations (simulate AI response caching)
    local cache_start=$(date +%s.%N)
    drush cache:set "ai_test_${provider}_$(date +%s)" "test_data" "ai_cache" 2>/dev/null >/dev/null || true
    drush cache:get "ai_test_${provider}_$(date +%s)" "ai_cache" 2>/dev/null >/dev/null || true
    local cache_end=$(date +%s.%N)
    local cache_duration=$(echo "$cache_end - $cache_start" | bc -l)
    
    test_results+='"cache_operation_time": '$cache_duration','
    
    local end_time=$(date +%s.%N)
    local total_duration=$(echo "$end_time - $start_time" | bc -l)
    
    test_results+='"total_test_time": '$total_duration','
    test_results+='"provider_healthy": true'
    
    test_results+="}"
    
    # Save individual provider results
    echo "$test_results" > "$AI_PERF_DIR/provider_${provider}_results.json"
    
    log_success "$provider provider tested in ${total_duration}s"
    echo "   Config load: ${config_duration}s"
    echo "   Module check: ${module_duration}s"
    echo "   Cache ops: ${cache_duration}s"
    
    return 0
}

# Function to benchmark AI integration performance
benchmark_ai_integration() {
    log_info "Running AI integration benchmarks..."
    
    local benchmark_results="{"
    local start_time=$(date +%s.%N)
    
    # Benchmark AI configuration loading
    local ai_config_start=$(date +%s.%N)
    local ai_configs=$(drush config:get ai.settings 2>/dev/null | wc -l || echo "0")
    local ai_config_end=$(date +%s.%N)
    local ai_config_duration=$(echo "$ai_config_end - $ai_config_start" | bc -l)
    
    benchmark_results+='"ai_config_load_time": '$ai_config_duration','
    benchmark_results+='"ai_config_lines": '$ai_configs','
    
    # Benchmark AI image alt text module (if available)
    local alt_text_start=$(date +%s.%N)
    local alt_text_configured=false
    
    if drush config:get ai_image_alt_text.settings 2>/dev/null >/dev/null; then
        alt_text_configured=true
    fi
    
    local alt_text_end=$(date +%s.%N)
    local alt_text_duration=$(echo "$alt_text_end - $alt_text_start" | bc -l)
    
    benchmark_results+='"alt_text_config_time": '$alt_text_duration','
    benchmark_results+='"alt_text_configured": '$alt_text_configured','
    
    # Benchmark AI content suggestions (if available)
    local suggestions_start=$(date +%s.%N)
    local suggestions_configured=false
    
    if drush config:get ai_content_suggestions.settings 2>/dev/null >/dev/null; then
        suggestions_configured=true
    fi
    
    local suggestions_end=$(date +%s.%N)
    local suggestions_duration=$(echo "$suggestions_end - $suggestions_start" | bc -l)
    
    benchmark_results+='"content_suggestions_config_time": '$suggestions_duration','
    benchmark_results+='"content_suggestions_configured": '$suggestions_configured','
    
    # Test AI logging performance
    local logging_start=$(date +%s.%N)
    local logging_configured=false
    
    if drush config:get ai_logging.settings 2>/dev/null >/dev/null; then
        logging_configured=true
    fi
    
    local logging_end=$(date +%s.%N)
    local logging_duration=$(echo "$logging_end - $logging_start" | bc -l)
    
    benchmark_results+='"ai_logging_config_time": '$logging_duration','
    benchmark_results+='"ai_logging_configured": '$logging_configured','
    
    local end_time=$(date +%s.%N)
    local total_benchmark_time=$(echo "$end_time - $start_time" | bc -l)
    
    benchmark_results+='"total_benchmark_time": '$total_benchmark_time
    benchmark_results+="}"
    
    echo "$benchmark_results" > "$AI_PERF_DIR/ai_integration_benchmark.json"
    
    # Update report if specified
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

with open('$AI_PERF_DIR/ai_integration_benchmark.json', 'r') as f:
    benchmark_data = json.load(f)

report_data['performance_tests']['ai_integration'] = benchmark_data

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
    
    log_success "AI integration benchmarked in ${total_benchmark_time}s"
}

# Function to test AI caching performance
test_ai_cache_performance() {
    log_info "Testing AI cache performance..."
    
    local cache_results="{"
    local start_time=$(date +%s.%N)
    
    # Test cache write performance
    local write_start=$(date +%s.%N)
    local test_data='{"ai_response": "This is a test AI response for performance testing", "model": "test", "timestamp": "'$(date -u +"%Y-%m-%dT%H:%M:%SZ")'"}'
    
    for i in {1..10}; do
        drush cache:set "ai_perf_test_$i" "$test_data" "ai_cache" 2>/dev/null >/dev/null || true
    done
    
    local write_end=$(date +%s.%N)
    local write_duration=$(echo "$write_end - $write_start" | bc -l)
    local avg_write_time=$(echo "scale=4; $write_duration / 10" | bc -l)
    
    cache_results+='"cache_write_time_10_items": '$write_duration','
    cache_results+='"avg_cache_write_time": '$avg_write_time','
    
    # Test cache read performance
    local read_start=$(date +%s.%N)
    
    for i in {1..10}; do
        drush cache:get "ai_perf_test_$i" "ai_cache" 2>/dev/null >/dev/null || true
    done
    
    local read_end=$(date +%s.%N)
    local read_duration=$(echo "$read_end - $read_start" | bc -l)
    local avg_read_time=$(echo "scale=4; $read_duration / 10" | bc -l)
    
    cache_results+='"cache_read_time_10_items": '$read_duration','
    cache_results+='"avg_cache_read_time": '$avg_read_time','
    
    # Test cache invalidation performance
    local invalidate_start=$(date +%s.%N)
    
    for i in {1..10}; do
        drush cache:delete "ai_perf_test_$i" "ai_cache" 2>/dev/null >/dev/null || true
    done
    
    local invalidate_end=$(date +%s.%N)
    local invalidate_duration=$(echo "$invalidate_end - $invalidate_start" | bc -l)
    local avg_invalidate_time=$(echo "scale=4; $invalidate_duration / 10" | bc -l)
    
    cache_results+='"cache_invalidate_time_10_items": '$invalidate_duration','
    cache_results+='"avg_cache_invalidate_time": '$avg_invalidate_time','
    
    local end_time=$(date +%s.%N)
    local total_cache_test_time=$(echo "$end_time - $start_time" | bc -l)
    
    cache_results+='"total_cache_test_time": '$total_cache_test_time
    cache_results+="}"
    
    echo "$cache_results" > "$AI_PERF_DIR/ai_cache_performance.json"
    
    # Update report if specified
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

with open('$AI_PERF_DIR/ai_cache_performance.json', 'r') as f:
    cache_data = json.load(f)

report_data['cache_performance'] = cache_data

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
    
    log_success "AI cache performance tested in ${total_cache_test_time}s"
    echo "   Avg write time: ${avg_write_time}s"
    echo "   Avg read time: ${avg_read_time}s"
    echo "   Avg invalidate time: ${avg_invalidate_time}s"
}

# Function to optimize AI integration
optimize_ai_integration() {
    log_info "Optimizing AI integration performance..."
    
    local optimizations=()
    
    # Create AI response cache configuration
    cat > "$AI_CACHE_DIR/ai-cache-config.php" << 'EOF'
<?php
/**
 * Optimized AI cache configuration
 */

$settings['cache']['bins']['ai_cache'] = 'cache.backend.database';
$settings['ai_cache_default_ttl'] = 3600; // 1 hour default TTL
$settings['ai_cache_max_size'] = 1000;     // Max cached responses

// AI provider timeout optimizations
$config['ai_provider_openai.settings']['timeout'] = 30;
$config['ai_provider_anthropic.settings']['timeout'] = 30;
$config['ai_provider_groq.settings']['timeout'] = 15; // Groq is typically faster

// AI logging optimizations
$config['ai_logging.settings']['log_requests'] = FALSE;
$config['ai_logging.settings']['log_responses'] = FALSE;
EOF
    
    log_success "Created optimized AI cache configuration"
    optimizations+=("AI cache configuration optimized")
    
    # Create AI performance monitoring script
    cat > "$AI_CACHE_DIR/ai-monitor.sh" << 'EOF'
#!/bin/bash

# AI performance monitoring daemon
LOG_FILE="/var/www/html/.ai-performance-cache/ai-performance.log"

while true; do
    TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    
    # Check AI module status
    AI_MODULES=$(drush pm:list --filter="ai" --status=enabled --format=json 2>/dev/null | jq '. | length' || echo "0")
    
    # Check cache hit rates (simulated)
    CACHE_HITS=$(drush cache:get "ai_monitor_test" "ai_cache" 2>/dev/null && echo "1" || echo "0")
    
    # Log performance data
    echo "$TIMESTAMP,ai_modules:$AI_MODULES,cache_test:$CACHE_HITS" >> "$LOG_FILE"
    
    # Keep only last 10000 entries
    tail -n 10000 "$LOG_FILE" > "$LOG_FILE.tmp" && mv "$LOG_FILE.tmp" "$LOG_FILE"
    
    sleep 60  # Check every minute
done
EOF
    
    chmod +x "$AI_CACHE_DIR/ai-monitor.sh"
    
    log_success "Created AI performance monitoring script"
    optimizations+=("AI performance monitoring daemon created")
    
    # Create AI provider failover configuration
    cat > "$AI_CACHE_DIR/ai-failover-config.yml" << 'EOF'
# AI provider failover configuration
ai_provider_settings:
  primary_provider: 'anthropic'
  fallback_providers:
    - 'openai'
    - 'groq'
  
  timeout_settings:
    anthropic: 30
    openai: 30
    groq: 15
  
  retry_settings:
    max_retries: 2
    retry_delay: 1
    
  cache_settings:
    enable_response_cache: true
    cache_ttl: 3600
    cache_size: 1000
EOF
    
    log_success "Created AI provider failover configuration"
    optimizations+=("AI provider failover configuration created")
    
    # Create AI optimization validation script
    cat > "$AI_PERF_DIR/validate-ai-optimizations.sh" << 'EOF'
#!/bin/bash

echo "🔍 Validating AI optimizations..."

# Check AI module performance
echo "📊 AI Module Status:"
drush pm:list --filter="ai" --status=enabled --format=table

# Test AI cache performance
echo ""
echo "🚀 Testing AI cache performance..."
START=$(date +%s.%N)
drush cache:set "ai_validation_test" '{"test": "data"}' "ai_cache" 2>/dev/null
drush cache:get "ai_validation_test" "ai_cache" >/dev/null 2>&1
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc -l)

echo "Cache operation time: ${DURATION}s"

# Check AI configuration
echo ""
echo "⚙️  AI Configuration Status:"
echo "OpenAI configured: $(drush config:get ai_provider_openai.settings api_key 2>/dev/null >/dev/null && echo 'Yes' || echo 'No')"
echo "Anthropic configured: $(drush config:get ai_provider_anthropic.settings api_key 2>/dev/null >/dev/null && echo 'Yes' || echo 'No')"
echo "Groq configured: $(drush config:get ai_provider_groq.settings api_key 2>/dev/null >/dev/null && echo 'Yes' || echo 'No')"

echo ""
echo "✅ AI optimization validation complete!"
EOF
    
    chmod +x "$AI_PERF_DIR/validate-ai-optimizations.sh"
    
    log_success "Created AI optimization validation script"
    optimizations+=("AI optimization validation script created")
    
    # Save optimizations to report
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

optimizations = $(printf '%s\n' "${optimizations[@]}" | jq -R . | jq -s .)
report_data['optimizations'] = optimizations

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
    
    log_success "AI integration optimized with ${#optimizations[@]} improvements"
}

# Function to generate AI performance recommendations
generate_ai_recommendations() {
    log_info "Generating AI performance recommendations..."
    
    python3 -c "
import json
import os

recommendations = []

# Load provider data if available
provider_file = '$AI_PERF_DIR/providers.json'
if os.path.exists(provider_file):
    with open(provider_file, 'r') as f:
        provider_data = json.load(f)
    
    total_providers = provider_data.get('total_providers', 0)
    
    if total_providers > 2:
        recommendations.append({
            'category': 'AI Provider Management',
            'priority': 'medium',
            'issue': f'Multiple AI providers configured ({total_providers})',
            'recommendation': 'Implement provider failover logic and monitor API costs across providers',
            'impact': 'Reduced API costs and improved reliability'
        })
    
    if total_providers == 0:
        recommendations.append({
            'category': 'AI Configuration',
            'priority': 'high',
            'issue': 'No AI providers configured',
            'recommendation': 'Configure at least one AI provider for AI features to work',
            'impact': 'Enable AI functionality'
        })

# Load cache performance data if available
cache_file = '$AI_PERF_DIR/ai_cache_performance.json'
if os.path.exists(cache_file):
    with open(cache_file, 'r') as f:
        cache_data = json.load(f)
    
    avg_write_time = cache_data.get('avg_cache_write_time', 0)
    avg_read_time = cache_data.get('avg_cache_read_time', 0)
    
    if avg_write_time > 0.1:
        recommendations.append({
            'category': 'AI Cache Performance',
            'priority': 'medium',
            'issue': f'Slow AI cache write performance ({avg_write_time:.3f}s average)',
            'recommendation': 'Consider using Redis or Memcache for AI response caching',
            'impact': 'Faster AI response caching'
        })
    
    if avg_read_time > 0.05:
        recommendations.append({
            'category': 'AI Cache Performance',
            'priority': 'low',
            'issue': f'AI cache read performance could be improved ({avg_read_time:.3f}s average)',
            'recommendation': 'Optimize cache backend or implement cache warming',
            'impact': 'Faster AI response retrieval'
        })

# General AI performance recommendations
recommendations.extend([
    {
        'category': 'AI Response Caching',
        'priority': 'high',
        'issue': 'AI responses should be cached to reduce API costs',
        'recommendation': 'Implement response caching with appropriate TTL based on content type',
        'impact': 'Reduced API costs and improved response times'
    },
    {
        'category': 'AI Request Optimization',
        'priority': 'medium',
        'issue': 'AI requests may benefit from batching and optimization',
        'recommendation': 'Implement request batching and prompt optimization strategies',
        'impact': 'Improved API efficiency and reduced costs'
    },
    {
        'category': 'AI Monitoring',
        'priority': 'medium',
        'issue': 'AI usage and performance should be monitored',
        'recommendation': 'Set up AI usage monitoring and alerting for API limits',
        'impact': 'Proactive AI performance management'
    }
])

# Save recommendations to file
with open('$AI_PERF_DIR/ai-recommendations.json', 'w') as f:
    json.dump(recommendations, f, indent=2)

print(f'Generated {len(recommendations)} AI performance recommendations')
for rec in recommendations[:3]:  # Show top 3
    print(f'• {rec[\"category\"]}: {rec[\"issue\"]}')
"
    
    # Update main report if specified
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

with open('$AI_PERF_DIR/ai-recommendations.json', 'r') as f:
    recommendations = json.load(f)

report_data['recommendations'] = recommendations

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
    
    log_success "AI performance recommendations generated"
}

# Main execution logic
log_info "Starting AI Performance Monitoring..."
log_info "Performance directory: $AI_PERF_DIR"
log_info "Cache directory: $AI_CACHE_DIR"

if [ -n "$REPORT_FILE" ]; then
    log_info "Report file: $REPORT_FILE"
fi

# Always detect providers first
detect_ai_providers

# Load provider information
if [ -f "$AI_PERF_DIR/providers.json" ]; then
    providers=($(python3 -c "
import json
with open('$AI_PERF_DIR/providers.json', 'r') as f:
    data = json.load(f)
print(' '.join(data['detected_providers']))
"))
else
    providers=()
fi

# Run operations based on options
if [ "$TEST_ALL" = true ]; then
    for provider in "${providers[@]}"; do
        test_ai_provider_performance "$provider"
    done
    if [ "$BENCHMARK_MODE" = true ]; then
        benchmark_ai_integration
    fi
    if [ "$CACHE_TEST" = true ]; then
        test_ai_cache_performance
    fi
elif [ -n "$PROVIDER" ]; then
    test_ai_provider_performance "$PROVIDER"
elif [ "$BENCHMARK_MODE" = true ]; then
    benchmark_ai_integration
elif [ "$CACHE_TEST" = true ]; then
    test_ai_cache_performance
fi

if [ "$OPTIMIZE_MODE" = true ]; then
    optimize_ai_integration
fi

# Always generate recommendations
generate_ai_recommendations

# Display summary
log_success "AI performance monitoring completed!"
echo ""
echo "🤖 AI Integration Summary:"
echo "   Providers detected: ${#providers[@]}"
if [ ${#providers[@]} -gt 0 ]; then
    echo "   Providers: ${providers[*]}"
fi
echo "   Performance artifacts: $AI_PERF_DIR"
echo "   Cache directory: $AI_CACHE_DIR"
if [ -n "$REPORT_FILE" ]; then
    echo "   Performance report: $REPORT_FILE"
fi
echo ""

# Show optimization tools if available
if [ -d "$AI_PERF_DIR" ] && [ "$(find "$AI_PERF_DIR" -name "*.sh" | wc -l)" -gt 0 ]; then
    log_info "Available AI optimization tools:"
    find "$AI_PERF_DIR" -name "*.sh" -exec basename {} \; | while read -r script; do
        echo "   🔧 $script"
    done
fi

echo ""
log_success "Use 'ddev ai-performance-monitor --test-all --benchmark --cache-test' for comprehensive testing"
log_success "Use 'ddev ai-performance-monitor --optimize' to apply AI performance optimizations"