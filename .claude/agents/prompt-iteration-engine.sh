#!/bin/bash
# Prompt Iteration Engine - Compounding Engineering Implementation
# Automatically improves AI prompts based on test results and failure analysis

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

PROMPT_DIR=".claude/knowledge/prompts"
ITERATION_DIR=".claude/knowledge/prompt-iterations"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")

# Create directories if they don't exist
mkdir -p "$PROMPT_DIR" "$ITERATION_DIR"

create_prompt_baseline() {
    local prompt_name="$1"
    local initial_prompt="$2"
    local test_criteria="$3"
    
    local baseline_file="$PROMPT_DIR/${prompt_name}_baseline.json"
    
    echo -e "${BLUE}ðŸ“ Creating prompt baseline: $prompt_name${NC}"
    
    cat > "$baseline_file" << EOF
{
    "prompt_name": "$prompt_name",
    "created_at": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
    "initial_prompt": $(echo "$initial_prompt" | jq -Rs .),
    "test_criteria": $(echo "$test_criteria" | jq -Rs .),
    "version": 1,
    "success_rate": 0,
    "iterations": [],
    "best_version": null,
    "learning_patterns": []
}
EOF
    
    echo -e "${GREEN}âœ… Baseline created: $baseline_file${NC}"
    echo "$baseline_file"
}

run_prompt_test_cycle() {
    local baseline_file="$1"
    local test_command="$2"
    local max_iterations="${3:-10}"
    local success_threshold="${4:-9}"
    
    echo -e "${PURPLE}ðŸ”„ Starting prompt iteration cycle...${NC}"
    
    local prompt_name=$(jq -r '.prompt_name' "$baseline_file")
    local current_prompt=$(jq -r '.initial_prompt' "$baseline_file")
    local test_criteria=$(jq -r '.test_criteria' "$baseline_file")
    
    for iteration in $(seq 1 $max_iterations); do
        echo -e "${YELLOW}Iteration $iteration/$max_iterations for $prompt_name${NC}"
        
        # Run test with current prompt
        local iteration_file="$ITERATION_DIR/${prompt_name}_iteration_${iteration}_${TIMESTAMP}.json"
        run_prompt_iteration "$current_prompt" "$test_command" "$iteration" "$iteration_file"
        
        # Analyze results
        local success_count=$(analyze_iteration_results "$iteration_file")
        
        echo -e "Success rate: $success_count/10"
        
        if [ "$success_count" -ge "$success_threshold" ]; then
            echo -e "${GREEN}âœ… Success threshold reached ($success_count/10)${NC}"
            update_best_prompt "$baseline_file" "$current_prompt" "$success_count" "$iteration"
            break
        fi
        
        # Generate improved prompt based on failures
        current_prompt=$(improve_prompt_from_failures "$iteration_file" "$current_prompt" "$test_criteria")
        
        if [ $iteration -eq $max_iterations ]; then
            echo -e "${RED}âš ï¸  Max iterations reached. Best success rate: $success_count/10${NC}"
        fi
    done
    
    # Extract final learning pattern
    extract_prompt_learning "$baseline_file"
}

run_prompt_iteration() {
    local prompt="$1"
    local test_command="$2"
    local iteration="$3"
    local output_file="$4"
    
    echo -e "${BLUE}ðŸ§ª Running prompt test iteration $iteration${NC}"
    
    # Initialize iteration record
    cat > "$output_file" << EOF
{
    "iteration": $iteration,
    "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
    "prompt_used": $(echo "$prompt" | jq -Rs .),
    "test_results": [],
    "success_count": 0,
    "failure_patterns": [],
    "improvements_identified": []
}
EOF
    
    # Run the test 10 times with current prompt
    local successes=0
    local results=()
    
    for test_run in $(seq 1 10); do
        echo -e "  Test run $test_run/10..."
        
        # Execute test with prompt (replace with actual implementation)
        local test_result=$(execute_prompt_test "$prompt" "$test_command" "$test_run")
        
        if echo "$test_result" | grep -q "SUCCESS"; then
            ((successes++))
            results+=("\"success\"")
        else
            results+=("\"failure: $(echo "$test_result" | tr '\n' ' ' | cut -c1-100)\"")
        fi
    done
    
    # Update results in JSON
    local results_json=$(printf '%s\n' "${results[@]}" | jq -s .)
    jq --argjson results "$results_json" --argjson count "$successes" \
        '.test_results = $results | .success_count = $count' \
        "$output_file" > "${output_file}.tmp" && mv "${output_file}.tmp" "$output_file"
    
    echo -e "${BLUE}ðŸ“Š Iteration $iteration completed: $successes/10 successes${NC}"
}

execute_prompt_test() {
    local prompt="$1"
    local test_command="$2"
    local test_run="$3"
    
    # This is where the actual prompt would be tested
    # For now, simulate based on prompt quality and randomness
    
    # Simple heuristic: longer, more specific prompts tend to work better
    local prompt_quality=$((${#prompt} / 50))  # Simple length-based quality metric
    local random_factor=$((RANDOM % 10))
    
    if [ $((prompt_quality + random_factor)) -gt 8 ]; then
        echo "SUCCESS: Test run $test_run passed with current prompt"
    else
        echo "FAILURE: Test run $test_run failed - prompt needs refinement"
    fi
}

analyze_iteration_results() {
    local iteration_file="$1"
    
    # Extract success count from iteration file
    jq -r '.success_count' "$iteration_file"
}

improve_prompt_from_failures() {
    local iteration_file="$1"
    local current_prompt="$2"
    local test_criteria="$3"
    
    echo -e "${YELLOW}ðŸ”§ Analyzing failures to improve prompt...${NC}"
    
    # Analyze failure patterns
    local failure_patterns=$(jq -r '.test_results[] | select(startswith("failure"))' "$iteration_file" 2>/dev/null || echo "")
    
    if [ -n "$failure_patterns" ]; then
        # Create improved prompt (in real implementation, this would use Claude)
        local improved_prompt="$current_prompt

# Improvements based on failure analysis:
# - Add more specific context for edge cases
# - Include examples of expected behavior
# - Clarify success criteria: $test_criteria

Please be more specific and provide concrete examples."
        
        echo -e "${GREEN}ðŸ”„ Prompt improved based on failure patterns${NC}"
        echo "$improved_prompt"
    else
        echo "$current_prompt"
    fi
}

update_best_prompt() {
    local baseline_file="$1"
    local best_prompt="$2"
    local success_rate="$3"
    local iteration="$4"
    
    echo -e "${GREEN}ðŸ† Updating best prompt (success rate: $success_rate/10)${NC}"
    
    jq --arg prompt "$best_prompt" --argjson rate "$success_rate" --argjson iter "$iteration" \
        '.best_version = {
            "prompt": $prompt,
            "success_rate": $rate,
            "iteration": $iter,
            "timestamp": "'$(date -u +"%Y-%m-%dT%H:%M:%SZ")'"
        }' \
        "$baseline_file" > "${baseline_file}.tmp" && mv "${baseline_file}.tmp" "$baseline_file"
}

extract_prompt_learning() {
    local baseline_file="$1"
    local prompt_name=$(jq -r '.prompt_name' "$baseline_file")
    
    echo -e "${PURPLE}ðŸŽ“ Extracting learning pattern for $prompt_name${NC}"
    
    local learning_file="$PROMPT_DIR/${prompt_name}_learning_${TIMESTAMP}.md"
    
    cat > "$learning_file" << EOF
# Prompt Learning Pattern: $prompt_name

**Generated**: $(date +"%Y-%m-%d %H:%M:%S")

## Original Challenge
$(jq -r '.test_criteria' "$baseline_file")

## Best Prompt Achieved
\`\`\`
$(jq -r '.best_version.prompt' "$baseline_file")
\`\`\`

## Success Rate
$(jq -r '.best_version.success_rate' "$baseline_file")/10 ($(jq -r '(.best_version.success_rate * 10)' "$baseline_file")%)

## Key Improvements Discovered
- [Auto-extracted patterns from iterations]
- More specific context reduces ambiguity
- Examples improve AI understanding
- Clear success criteria essential

## CLAUDE.md Integration
This prompt pattern should be documented in CLAUDE.md for reuse in similar scenarios.

## Reusable Pattern
For similar challenges, use this prompt structure:
1. Clear context setting
2. Specific examples
3. Explicit success criteria  
4. Failure pattern avoidance
EOF
    
    # Update CLAUDE.md with the learning
    update_claude_md_with_prompt_pattern "$learning_file"
    
    echo -e "${GREEN}âœ… Learning pattern documented: $learning_file${NC}"
}

update_claude_md_with_prompt_pattern() {
    local learning_file="$1"
    local prompt_name=$(basename "$learning_file" | sed 's/_learning_.*\.md//')
    
    echo -e "${BLUE}ðŸ“ Adding prompt pattern to CLAUDE.md...${NC}"
    
    # Add to AI Integration Patterns section
    local pattern_entry="- **$prompt_name Prompt Pattern** â†’ Achieved $(grep "Success Rate" "$learning_file" | cut -d' ' -f3) success rate â†’ Pattern: Context + Examples + Clear Criteria"
    
    if grep -q "#### AI Integration Patterns" CLAUDE.md; then
        sed -i "/#### AI Integration Patterns/a\\
$pattern_entry" CLAUDE.md
    else
        # Add the section if it doesn't exist
        sed -i "/## ðŸ§  Compounding Knowledge Sections/a\\
\\
#### AI Integration Patterns\\
$pattern_entry" CLAUDE.md
    fi
    
    echo -e "${GREEN}âœ… CLAUDE.md updated with prompt pattern${NC}"
}

# Main execution
main() {
    local prompt_name="${1:-}"
    local initial_prompt="${2:-}"
    local test_command="${3:-}"
    local test_criteria="${4:-General AI task success}"
    
    if [ -z "$prompt_name" ] || [ -z "$initial_prompt" ] || [ -z "$test_command" ]; then
        echo -e "${RED}Usage: $0 <prompt_name> <initial_prompt> <test_command> [test_criteria]${NC}"
        echo ""
        echo "Examples:"
        echo "  $0 'frustration-detector' 'Detect user frustration in text' 'test-frustration.sh' 'Accurately identify frustrated users'"
        echo "  $0 'code-reviewer' 'Review this code for issues' 'run-review-test.sh' 'Find security and quality issues'"
        exit 1
    fi
    
    echo -e "${PURPLE}ðŸš€ Starting Prompt Iteration Engine${NC}"
    echo -e "${BLUE}Prompt: $prompt_name${NC}"
    echo -e "${BLUE}Test Command: $test_command${NC}"
    echo -e "${BLUE}Criteria: $test_criteria${NC}"
    
    # Create baseline
    baseline_file=$(create_prompt_baseline "$prompt_name" "$initial_prompt" "$test_criteria")
    
    # Run iteration cycle
    run_prompt_test_cycle "$baseline_file" "$test_command" 10 9
    
    echo -e "${PURPLE}ðŸŽ¯ Prompt iteration cycle complete!${NC}"
    echo -e "${GREEN}Best prompt saved and learning documented${NC}"
}

# Run if called directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi