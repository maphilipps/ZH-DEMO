#!/usr/bin/env bash

## #ddev-generated
## Description: Optimize database and content performance for Drupal 11 CMS
## Usage: db-performance-optimizer [options]
## Example: "ddev db-performance-optimizer --analyze"
##   or "ddev db-performance-optimizer --optimize-all"
##   or "ddev db-performance-optimizer --media-optimize"
##   or "ddev db-performance-optimizer --query-analysis --report=/tmp/db-perf.json"

set -e

# Color output functions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${BLUE}[DB-PERF]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_db() {
    echo -e "${PURPLE}[DATABASE]${NC} $1"
}

# Database performance optimization settings
DB_PERF_DIR="/tmp/adesso-cms-db-perf"
DB_CACHE_DIR="/var/www/html/.db-performance-cache"
REPORT_FILE=""

# Parse command line arguments
ANALYZE_MODE=false
OPTIMIZE_ALL=false
MEDIA_OPTIMIZE=false
QUERY_ANALYSIS=false
BENCHMARK_MODE=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --analyze)
            ANALYZE_MODE=true
            shift
            ;;
        --optimize-all)
            OPTIMIZE_ALL=true
            shift
            ;;
        --media-optimize)
            MEDIA_OPTIMIZE=true
            shift
            ;;
        --query-analysis)
            QUERY_ANALYSIS=true
            shift
            ;;
        --benchmark)
            BENCHMARK_MODE=true
            shift
            ;;
        --report=*)
            REPORT_FILE="${1#*=}"
            shift
            ;;
        *)
            log_error "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Create directories
mkdir -p "$DB_PERF_DIR" "$DB_CACHE_DIR"

# Initialize report if specified
if [ -n "$REPORT_FILE" ]; then
    cat > "$REPORT_FILE" << 'EOF'
{
  "timestamp": "",
  "database_analysis": {},
  "content_analysis": {},
  "media_analysis": {},
  "query_performance": {},
  "optimizations": [],
  "recommendations": []
}
EOF
    
    # Update timestamp
    python3 -c "
import json
from datetime import datetime

with open('$REPORT_FILE', 'r') as f:
    data = json.load(f)

data['timestamp'] = datetime.utcnow().isoformat() + 'Z'

with open('$REPORT_FILE', 'w') as f:
    json.dump(data, f, indent=2)
"
fi

# Function to analyze database performance
analyze_database() {
    log_db "Analyzing database performance..."
    
    local start_time=$(date +%s.%N)
    local analysis_results="{"
    
    # Get database size and table information
    local db_size=$(drush sql:query "SELECT ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) AS 'DB Size in MB' FROM information_schema.tables WHERE table_schema='db';" 2>/dev/null | tail -n1 || echo "0")
    analysis_results+='"database_size_mb": '$db_size','
    
    # Get table count
    local table_count=$(drush sql:query "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='db';" 2>/dev/null | tail -n1 || echo "0")
    analysis_results+='"table_count": '$table_count','
    
    # Get largest tables
    local largest_tables=$(drush sql:query "SELECT table_name, ROUND(((data_length + index_length) / 1024 / 1024), 2) AS 'Size in MB' FROM information_schema.TABLES WHERE table_schema = 'db' ORDER BY (data_length + index_length) DESC LIMIT 5;" 2>/dev/null | tail -n+2 | wc -l || echo "0")
    analysis_results+='"largest_tables_analyzed": '$largest_tables','
    
    # Analyze content entities
    local node_count=$(drush sql:query "SELECT COUNT(*) FROM node;" 2>/dev/null | tail -n1 || echo "0")
    local media_count=$(drush sql:query "SELECT COUNT(*) FROM media;" 2>/dev/null | tail -n1 || echo "0")
    local paragraph_count=$(drush sql:query "SELECT COUNT(*) FROM paragraphs_item;" 2>/dev/null | tail -n1 || echo "0")
    local user_count=$(drush sql:query "SELECT COUNT(*) FROM users;" 2>/dev/null | tail -n1 || echo "0")
    
    analysis_results+='"node_count": '$node_count','
    analysis_results+='"media_count": '$media_count','
    analysis_results+='"paragraph_count": '$paragraph_count','
    analysis_results+='"user_count": '$user_count','
    
    # Analyze database configuration
    local innodb_buffer_pool=$(drush sql:query "SHOW VARIABLES LIKE 'innodb_buffer_pool_size';" 2>/dev/null | tail -n1 | awk '{print $2}' || echo "0")
    local query_cache_size=$(drush sql:query "SHOW VARIABLES LIKE 'query_cache_size';" 2>/dev/null | tail -n1 | awk '{print $2}' || echo "0")
    
    analysis_results+='"innodb_buffer_pool_size": "'$innodb_buffer_pool'",'
    analysis_results+='"query_cache_size": "'$query_cache_size'",'
    
    local end_time=$(date +%s.%N)
    local analysis_duration=$(echo "$end_time - $start_time" | bc -l)
    
    analysis_results+='"analysis_time": '$analysis_duration
    analysis_results+="}"
    
    echo "$analysis_results" > "$DB_PERF_DIR/database_analysis.json"
    
    # Update report if specified
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

with open('$DB_PERF_DIR/database_analysis.json', 'r') as f:
    db_data = json.load(f)

report_data['database_analysis'] = db_data

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
    
    log_success "Database analyzed in ${analysis_duration}s"
    echo "   Database size: ${db_size} MB"
    echo "   Tables: $table_count"
    echo "   Nodes: $node_count"
    echo "   Media items: $media_count"
    echo "   Paragraphs: $paragraph_count"
}

# Function to analyze content performance
analyze_content_performance() {
    log_info "Analyzing content performance..."
    
    local start_time=$(date +%s.%N)
    local content_results="{"
    
    # Analyze content types
    local content_types=$(drush sql:query "SELECT type, COUNT(*) as count FROM node GROUP BY type;" 2>/dev/null | tail -n+2 | wc -l || echo "0")
    content_results+='"content_types": '$content_types','
    
    # Analyze media types
    local media_types=$(drush sql:query "SELECT bundle, COUNT(*) as count FROM media GROUP BY bundle;" 2>/dev/null | tail -n+2 | wc -l || echo "0")
    content_results+='"media_types": '$media_types','
    
    # Analyze paragraph types
    local paragraph_types=$(drush sql:query "SELECT type, COUNT(*) as count FROM paragraphs_item GROUP BY type;" 2>/dev/null | tail -n+2 | wc -l || echo "0")
    content_results+='"paragraph_types": '$paragraph_types','
    
    # Analyze field data tables
    local field_tables=$(drush sql:query "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='db' AND table_name LIKE 'node__field_%';" 2>/dev/null | tail -n1 || echo "0")
    content_results+='"node_field_tables": '$field_tables','
    
    local media_field_tables=$(drush sql:query "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='db' AND table_name LIKE 'media__field_%';" 2>/dev/null | tail -n1 || echo "0")
    content_results+='"media_field_tables": '$media_field_tables','
    
    local paragraph_field_tables=$(drush sql:query "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='db' AND table_name LIKE 'paragraph__field_%';" 2>/dev/null | tail -n1 || echo "0")
    content_results+='"paragraph_field_tables": '$paragraph_field_tables','
    
    # Test content query performance
    local query_start=$(date +%s.%N)
    drush sql:query "SELECT n.nid, n.title FROM node_field_data n WHERE n.status = 1 LIMIT 10;" >/dev/null 2>&1
    local query_end=$(date +%s.%N)
    local query_duration=$(echo "$query_end - $query_start" | bc -l)
    
    content_results+='"sample_content_query_time": '$query_duration','
    
    local end_time=$(date +%s.%N)
    local content_analysis_duration=$(echo "$end_time - $start_time" | bc -l)
    
    content_results+='"content_analysis_time": '$content_analysis_duration
    content_results+="}"
    
    echo "$content_results" > "$DB_PERF_DIR/content_analysis.json"
    
    # Update report if specified
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

with open('$DB_PERF_DIR/content_analysis.json', 'r') as f:
    content_data = json.load(f)

report_data['content_analysis'] = content_data

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
    
    log_success "Content performance analyzed in ${content_analysis_duration}s"
    echo "   Content types: $content_types"
    echo "   Media types: $media_types"
    echo "   Paragraph types: $paragraph_types"
    echo "   Sample query time: ${query_duration}s"
}

# Function to optimize media performance
optimize_media_performance() {
    log_info "Optimizing media performance..."
    
    local start_time=$(date +%s.%N)
    local optimizations=()
    
    # Create media optimization script
    cat > "$DB_PERF_DIR/media-optimizer.sh" << 'EOF'
#!/bin/bash

# Media performance optimization script

echo "🖼️  Starting media performance optimization..."

# 1. Clear image style cache to force regeneration with optimized settings
echo "Clearing image style cache..."
drush cache:clear image

# 2. Optimize image styles - remove unused ones
echo "Analyzing image styles..."
TOTAL_STYLES=$(drush image-style:list --format=json 2>/dev/null | jq '. | length' || echo "0")
echo "Found $TOTAL_STYLES image styles configured"

if [ "$TOTAL_STYLES" -gt 30 ]; then
    echo "⚠️  Warning: Large number of image styles may impact performance"
    echo "Consider reviewing and consolidating image styles"
fi

# 3. Pre-generate common image styles for better performance
echo "Pre-generating common image styles..."
COMMON_STYLES=("thumbnail" "medium" "large" "card" "hero")

for style in "${COMMON_STYLES[@]}"; do
    echo "Processing style: $style"
    drush image-style:flush "$style" 2>/dev/null || echo "Style $style not found"
done

# 4. Analyze media usage
echo "Analyzing media usage..."
TOTAL_MEDIA=$(drush sql:query "SELECT COUNT(*) FROM media;" 2>/dev/null | tail -n1 || echo "0")
ORPHANED_FILES=$(drush sql:query "SELECT COUNT(*) FROM file_managed WHERE status = 0;" 2>/dev/null | tail -n1 || echo "0")

echo "📊 Media Statistics:"
echo "   Total media items: $TOTAL_MEDIA"
echo "   Potentially orphaned files: $ORPHANED_FILES"

# 5. Clean up orphaned files (if safe to do so)
if [ "$ORPHANED_FILES" -gt 0 ]; then
    echo "Found $ORPHANED_FILES orphaned files"
    echo "Run 'drush file:orphans' to review and clean up"
fi

echo "✅ Media optimization completed!"
EOF
    
    chmod +x "$DB_PERF_DIR/media-optimizer.sh"
    
    # Run the media optimizer
    "$DB_PERF_DIR/media-optimizer.sh"
    
    optimizations+=("Media performance optimizer created and executed")
    
    # Create media monitoring configuration
    cat > "$DB_CACHE_DIR/media-monitor-config.json" << 'EOF'
{
  "image_style_monitoring": {
    "max_styles": 30,
    "common_styles": ["thumbnail", "medium", "large", "card", "hero"],
    "check_interval": 3600
  },
  "file_cleanup": {
    "orphan_file_threshold": 100,
    "cleanup_interval": 86400,
    "backup_before_cleanup": true
  },
  "performance_thresholds": {
    "image_generation_max_time": 5.0,
    "media_query_max_time": 0.5,
    "max_media_items_per_page": 50
  }
}
EOF
    
    optimizations+=("Media monitoring configuration created")
    
    # Create database index optimization for media
    cat > "$DB_PERF_DIR/media-db-indexes.sql" << 'EOF'
-- Media performance database indexes

-- Index for media bundle queries
CREATE INDEX idx_media_bundle_status ON media (bundle, status);

-- Index for file usage queries  
CREATE INDEX idx_file_managed_status_uri ON file_managed (status, uri(255));

-- Index for media field queries (common patterns)
CREATE INDEX idx_media_field_data_bundle_created ON media_field_data (bundle, created);
CREATE INDEX idx_media_field_data_status_changed ON media_field_data (status, changed);

-- Index for paragraph media references
CREATE INDEX idx_paragraph_field_media ON paragraph__field_media (field_media_target_id);

-- Index for node media references
CREATE INDEX idx_node_field_featured_image ON node__field_featured_image (field_featured_image_target_id);

-- Analyze tables after index creation
ANALYZE TABLE media, file_managed, media_field_data;
EOF
    
    log_info "Database indexes for media performance created"
    log_warning "Review and manually apply indexes in: $DB_PERF_DIR/media-db-indexes.sql"
    
    optimizations+=("Media database indexes created for manual review")
    
    local end_time=$(date +%s.%N)
    local optimization_duration=$(echo "$end_time - $start_time" | bc -l)
    
    # Analyze media performance after optimization
    local media_count=$(drush sql:query "SELECT COUNT(*) FROM media;" 2>/dev/null | tail -n1 || echo "0")
    local image_styles=$(drush image-style:list --format=json 2>/dev/null | python3 -c "
import json
import sys
try:
    data = json.load(sys.stdin)
    print(len(data))
except:
    print('0')
" || echo "0")
    
    local media_results="{"
    media_results+='"optimization_duration": '$optimization_duration','
    media_results+='"media_count": '$media_count','
    media_results+='"image_styles": '$image_styles','
    media_results+='"optimizations_applied": '$(printf '%s\n' "${optimizations[@]}" | jq -R . | jq -s . | jq length)
    media_results+="}"
    
    echo "$media_results" > "$DB_PERF_DIR/media_optimization_results.json"
    
    # Update report if specified
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

with open('$DB_PERF_DIR/media_optimization_results.json', 'r') as f:
    media_data = json.load(f)

report_data['media_analysis'] = media_data

optimizations = $(printf '%s\n' "${optimizations[@]}" | jq -R . | jq -s .)
if 'optimizations' not in report_data:
    report_data['optimizations'] = []
report_data['optimizations'].extend(optimizations)

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
    
    log_success "Media performance optimized in ${optimization_duration}s"
    echo "   Media items: $media_count"
    echo "   Image styles: $image_styles"
    echo "   Optimizations applied: ${#optimizations[@]}"
}

# Function to analyze query performance
analyze_query_performance() {
    log_db "Analyzing query performance..."
    
    local start_time=$(date +%s.%N)
    
    # Create query performance test script
    cat > "$DB_PERF_DIR/query-performance-test.sh" << 'EOF'
#!/bin/bash

# Query performance testing script

echo "🔍 Testing query performance..."

# Test common query patterns
QUERIES=(
    "SELECT COUNT(*) FROM node WHERE status = 1"
    "SELECT n.nid, n.title FROM node_field_data n WHERE n.status = 1 AND n.type = 'page' LIMIT 10"
    "SELECT m.mid, m.name FROM media_field_data m WHERE m.status = 1 LIMIT 10"
    "SELECT p.id, p.type FROM paragraphs_item p LIMIT 10"
    "SELECT u.uid, u.name FROM users u WHERE u.status = 1 LIMIT 10"
)

echo "📊 Query Performance Results:"
echo "=============================="

for query in "${QUERIES[@]}"; do
    echo "Query: $query"
    
    START=$(date +%s.%N)
    drush sql:query "$query" >/dev/null 2>&1
    END=$(date +%s.%N)
    
    DURATION=$(echo "$END - $START" | bc -l)
    echo "Duration: ${DURATION}s"
    echo "-------------------------------"
done

# Test complex queries
echo ""
echo "🔬 Complex Query Tests:"
echo "======================"

# Join query test
echo "Testing node-media join query..."
START=$(date +%s.%N)
drush sql:query "SELECT n.nid, n.title, m.name FROM node_field_data n LEFT JOIN node__field_featured_image fi ON n.nid = fi.entity_id LEFT JOIN media_field_data m ON fi.field_featured_image_target_id = m.mid WHERE n.status = 1 LIMIT 5;" >/dev/null 2>&1
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc -l)
echo "Node-Media join duration: ${DURATION}s"

# Paragraph reference query
echo "Testing paragraph reference query..."
START=$(date +%s.%N)
drush sql:query "SELECT n.nid, p.type FROM node_field_data n JOIN node__field_content fc ON n.nid = fc.entity_id JOIN paragraphs_item p ON fc.field_content_target_id = p.id WHERE n.status = 1 LIMIT 5;" >/dev/null 2>&1
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc -l)
echo "Paragraph reference duration: ${DURATION}s"

echo ""
echo "✅ Query performance testing completed!"
EOF
    
    chmod +x "$DB_PERF_DIR/query-performance-test.sh"
    
    # Run query performance tests
    "$DB_PERF_DIR/query-performance-test.sh" > "$DB_PERF_DIR/query-performance-results.txt" 2>&1
    
    local end_time=$(date +%s.%N)
    local query_analysis_duration=$(echo "$end_time - $start_time" | bc -l)
    
    # Extract performance metrics from results
    local basic_query_count=$(grep "Duration:" "$DB_PERF_DIR/query-performance-results.txt" | wc -l || echo "0")
    local complex_query_count=$(grep "duration:" "$DB_PERF_DIR/query-performance-results.txt" | wc -l || echo "0")
    
    local query_results="{"
    query_results+='"query_analysis_duration": '$query_analysis_duration','
    query_results+='"basic_queries_tested": '$basic_query_count','
    query_results+='"complex_queries_tested": '$complex_query_count','
    query_results+='"results_file": "'$DB_PERF_DIR/query-performance-results.txt'"'
    query_results+="}"
    
    echo "$query_results" > "$DB_PERF_DIR/query_performance.json"
    
    # Update report if specified
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

with open('$DB_PERF_DIR/query_performance.json', 'r') as f:
    query_data = json.load(f)

report_data['query_performance'] = query_data

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
    
    log_success "Query performance analyzed in ${query_analysis_duration}s"
    echo "   Basic queries tested: $basic_query_count"
    echo "   Complex queries tested: $complex_query_count"
    echo "   Results: $DB_PERF_DIR/query-performance-results.txt"
}

# Function to optimize database configuration
optimize_database_config() {
    log_db "Creating database optimization recommendations..."
    
    # Create database optimization configuration
    cat > "$DB_CACHE_DIR/database-optimization.cnf" << 'EOF'
# MySQL/MariaDB optimization for Drupal 11 CMS
# Add to /etc/mysql/conf.d/ or similar location

[mysqld]
# Memory optimizations
innodb_buffer_pool_size = 256M
innodb_log_file_size = 64M
innodb_log_buffer_size = 16M
innodb_flush_log_at_trx_commit = 2

# Connection optimizations
max_connections = 200
max_connect_errors = 1000000

# Query cache (if available)
query_cache_type = 1
query_cache_size = 64M
query_cache_limit = 2M

# Table optimization
table_open_cache = 2000
table_definition_cache = 1400

# Temporary table optimization
tmp_table_size = 64M
max_heap_table_size = 64M

# MyISAM optimization
key_buffer_size = 32M

# Logging (disable for production performance)
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 2
EOF
    
    # Create database maintenance script
    cat > "$DB_PERF_DIR/db-maintenance.sh" << 'EOF'
#!/bin/bash

# Database maintenance script for performance

echo "🔧 Starting database maintenance..."

# 1. Analyze and optimize tables
echo "Analyzing and optimizing database tables..."
drush sql:query "ANALYZE TABLE node, node_field_data, media, media_field_data, paragraphs_item;"
drush sql:query "OPTIMIZE TABLE cache_bootstrap, cache_config, cache_container, cache_data, cache_default, cache_discovery, cache_dynamic_page_cache, cache_entity, cache_menu, cache_render, cache_toolbar;"

# 2. Clean up old cache entries
echo "Cleaning up cache tables..."
drush cache:rebuild

# 3. Clean up old log entries (if applicable)
echo "Cleaning up log entries..."
drush sql:query "DELETE FROM watchdog WHERE timestamp < UNIX_TIMESTAMP(DATE_SUB(NOW(), INTERVAL 30 DAY));" 2>/dev/null || echo "Watchdog cleanup skipped"

# 4. Update table statistics
echo "Updating table statistics..."
drush sql:query "ANALYZE TABLE node, media, paragraphs_item, users;"

# 5. Check database size
DB_SIZE=$(drush sql:query "SELECT ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) FROM information_schema.tables WHERE table_schema='db';" | tail -n1)
echo "Current database size: ${DB_SIZE} MB"

# 6. Vacuum and defragment (if needed)
echo "Database maintenance completed!"
echo "Database size after maintenance: ${DB_SIZE} MB"
EOF
    
    chmod +x "$DB_PERF_DIR/db-maintenance.sh"
    
    log_success "Database optimization configuration created"
    echo "   MySQL config: $DB_CACHE_DIR/database-optimization.cnf"
    echo "   Maintenance script: $DB_PERF_DIR/db-maintenance.sh"
}

# Function to generate database performance recommendations
generate_db_recommendations() {
    log_info "Generating database performance recommendations..."
    
    python3 -c "
import json
import os

recommendations = []

# Load database analysis if available
db_analysis_file = '$DB_PERF_DIR/database_analysis.json'
if os.path.exists(db_analysis_file):
    with open(db_analysis_file, 'r') as f:
        db_data = json.load(f)
    
    db_size = db_data.get('database_size_mb', 0)
    node_count = db_data.get('node_count', 0)
    media_count = db_data.get('media_count', 0)
    
    if db_size > 500:
        recommendations.append({
            'category': 'Database Size',
            'priority': 'high',
            'issue': f'Large database size ({db_size} MB)',
            'recommendation': 'Review data retention policies, archive old content, and optimize large tables',
            'impact': 'Improved backup speed and query performance'
        })
    
    if media_count > 1000:
        recommendations.append({
            'category': 'Media Management',
            'priority': 'medium',
            'issue': f'High number of media items ({media_count})',
            'recommendation': 'Implement media cleanup policies and consider cloud storage integration',
            'impact': 'Reduced database size and improved media performance'
        })
    
    if node_count > 10000:
        recommendations.append({
            'category': 'Content Management',
            'priority': 'medium',
            'issue': f'Large number of content items ({node_count})',
            'recommendation': 'Consider content archiving and implement search indexing optimization',
            'impact': 'Improved content query performance'
        })

# Load content analysis if available
content_analysis_file = '$DB_PERF_DIR/content_analysis.json'
if os.path.exists(content_analysis_file):
    with open(content_analysis_file, 'r') as f:
        content_data = json.load(f)
    
    field_tables = content_data.get('node_field_tables', 0) + content_data.get('media_field_tables', 0) + content_data.get('paragraph_field_tables', 0)
    
    if field_tables > 100:
        recommendations.append({
            'category': 'Database Schema',
            'priority': 'low',
            'issue': f'High number of field tables ({field_tables})',
            'recommendation': 'Review field usage and consider field consolidation where appropriate',
            'impact': 'Simplified database schema and improved join performance'
        })

# General database performance recommendations
recommendations.extend([
    {
        'category': 'Database Caching',
        'priority': 'high',
        'issue': 'Database query caching optimization needed',
        'recommendation': 'Implement Redis or Memcache for database query caching',
        'impact': 'Significantly improved query response times'
    },
    {
        'category': 'Database Indexing',
        'priority': 'high',
        'issue': 'Database indexes may need optimization',
        'recommendation': 'Review and optimize database indexes for common query patterns',
        'impact': 'Faster database queries and improved page load times'
    },
    {
        'category': 'Database Maintenance',
        'priority': 'medium',
        'issue': 'Regular database maintenance needed',
        'recommendation': 'Implement automated database maintenance including table optimization and cleanup',
        'impact': 'Consistent database performance and reduced storage usage'
    },
    {
        'category': 'Entity Caching',
        'priority': 'high',
        'issue': 'Entity caching optimization needed',
        'recommendation': 'Configure and optimize Drupal entity caching for better performance',
        'impact': 'Reduced database load and faster content rendering'
    }
])

# Save recommendations
with open('$DB_PERF_DIR/db-recommendations.json', 'w') as f:
    json.dump(recommendations, f, indent=2)

print(f'Generated {len(recommendations)} database performance recommendations')
for rec in recommendations[:3]:  # Show top 3
    print(f'• {rec[\"category\"]}: {rec[\"issue\"]}')
"
    
    # Update main report if specified
    if [ -n "$REPORT_FILE" ]; then
        python3 -c "
import json

with open('$REPORT_FILE', 'r') as f:
    report_data = json.load(f)

with open('$DB_PERF_DIR/db-recommendations.json', 'r') as f:
    recommendations = json.load(f)

if 'recommendations' not in report_data:
    report_data['recommendations'] = []
report_data['recommendations'].extend(recommendations)

with open('$REPORT_FILE', 'w') as f:
    json.dump(report_data, f, indent=2)
"
    fi
    
    log_success "Database performance recommendations generated"
}

# Main execution logic
log_info "Starting Database Performance Optimization..."
log_info "Performance directory: $DB_PERF_DIR"
log_info "Cache directory: $DB_CACHE_DIR"

if [ -n "$REPORT_FILE" ]; then
    log_info "Report file: $REPORT_FILE"
fi

# Run operations based on options
if [ "$ANALYZE_MODE" = true ]; then
    analyze_database
    analyze_content_performance
    if [ "$QUERY_ANALYSIS" = true ]; then
        analyze_query_performance
    fi
elif [ "$OPTIMIZE_ALL" = true ]; then
    analyze_database
    analyze_content_performance
    optimize_media_performance
    optimize_database_config
    if [ "$QUERY_ANALYSIS" = true ]; then
        analyze_query_performance
    fi
elif [ "$MEDIA_OPTIMIZE" = true ]; then
    optimize_media_performance
elif [ "$QUERY_ANALYSIS" = true ]; then
    analyze_query_performance
else
    log_info "No specific action specified. Running analysis..."
    analyze_database
fi

# Always generate recommendations
generate_db_recommendations

# Display summary
log_success "Database performance optimization completed!"
echo ""
echo "🗄️  Database Summary:"
if [ -f "$DB_PERF_DIR/database_analysis.json" ]; then
    python3 -c "
import json
with open('$DB_PERF_DIR/database_analysis.json', 'r') as f:
    data = json.load(f)
print(f'   Database size: {data.get(\"database_size_mb\", \"N/A\")} MB')
print(f'   Tables: {data.get(\"table_count\", \"N/A\")}')
print(f'   Nodes: {data.get(\"node_count\", \"N/A\")}')
print(f'   Media: {data.get(\"media_count\", \"N/A\")}')
"
fi
echo "   Artifacts: $DB_PERF_DIR"
echo "   Config: $DB_CACHE_DIR"
if [ -n "$REPORT_FILE" ]; then
    echo "   Report: $REPORT_FILE"
fi
echo ""

# Show available tools
if [ -d "$DB_PERF_DIR" ] && [ "$(ls -A "$DB_PERF_DIR")" ]; then
    log_info "Available database tools:"
    find "$DB_PERF_DIR" -name "*.sh" -exec basename {} \; | while read -r script; do
        echo "   🔧 $script"
    done
fi

echo ""
log_success "Use 'ddev db-performance-optimizer --analyze --query-analysis' for comprehensive analysis"
log_success "Use 'ddev db-performance-optimizer --optimize-all' to apply all optimizations"